{"nbformat_minor": 2, "cells": [{"source": "# HW3_Part2", "cell_type": "markdown", "metadata": {}}, {"source": "### Yanis Daci ", "cell_type": "markdown", "metadata": {}}, {"source": "# Importation of data ", "cell_type": "markdown", "metadata": {}}, {"source": "First of all, before answering the homework questions, we have to import the data and create usable DataFrame.", "cell_type": "markdown", "metadata": {}}, {"source": "We imported the DBLP files into our Hadoop File System.", "cell_type": "markdown", "metadata": {}}, {"source": "Then, we have to create a schema for each table in order to obtain DataFrame with suitable columns. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "from pyspark.sql.types import *", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>37</td><td>application_1544525833195_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-m2yani.1bdghrepeb4etfuyomqefrb24a.ax.internal.cloudapp.net:8088/proxy/application_1544525833195_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-m2yani.1bdghrepeb4etfuyomqefrb24a.ax.internal.cloudapp.net:30060/node/containerlogs/container_e08_1544525833195_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "schema_authors = StructType([\n\n    StructField(\"id\",IntegerType(),True),\n    StructField(\"name\",StringType(),True)])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "schema_papers = StructType([\n\n    StructField(\"id\",IntegerType(),True),\n    StructField(\"name\",StringType(),False),\n    StructField(\"venue\",IntegerType(),True),\n    StructField(\"pages\",StringType(),True),\n    StructField(\"url\",StringType(),True)])", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "schema_paperauths = StructType([\n\n    StructField(\"paperid\",IntegerType(),True),\n    StructField(\"authid\",IntegerType(),True)])", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 5, "cell_type": "code", "source": "schema_venue = StructType([\n\n    StructField(\"id\",IntegerType(),True),\n    StructField(\"name\",StringType(),False),\n    StructField(\"year\",IntegerType(),False),\n    StructField(\"school\",StringType(),True),\n    StructField(\"volume\",StringType(),True),\n    StructField(\"number\",StringType(),True),\n    StructField(\"type\",IntegerType(),False)])", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Now, we can create DataFrames based on our schemas :", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "authors = spark.read.csv(\"wasb:///authors.tsv\",sep='\\t',schema = schema_authors) ", "outputs": [], "metadata": {"collapsed": true}}, {"source": "When we use the **read.csv** function, we will obtain a dataframe.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "authors.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+--------------------+\n|     id|                name|\n+-------+--------------------+\n|1112044|     Dennis Matthews|\n| 370385|      Neyre Tekbiyik|\n| 416706|       Rosa Manrique|\n|1308005|      William Pisano|\n| 577458|   Dragan S. Popovic|\n| 428257|       G. M. Nijssen|\n|1634371|     Abdellah Mouhou|\n|1319557|       Brett Dolezal|\n|1341966|Suhas Haribhau Patil|\n|1499227|     Mohamed Ben Ali|\n| 741915|  Ganesan Kannabiran|\n| 259828|       K. B. McAuley|\n| 328368|    Hrushikesh Garud|\n|1285089|     Takamasa Adachi|\n|1250421|           Wenyu Zhu|\n| 505346|Krzysztof Jakub K...|\n| 753175|       Ray J. Meyers|\n|1300662|  Aleksandar Corovic|\n| 818661|          E. P\u00e9rinel|\n| 349077|          C. P. Gill|\n+-------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "paperauths = spark.read.csv(\"wasb:///paperauths.tsv\",sep='\\t',schema = schema_paperauths)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 9, "cell_type": "code", "source": "paperauths.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+-------+\n|paperid| authid|\n+-------+-------+\n|1647633|  14170|\n|1125503| 848309|\n|1711987|1184704|\n|2602130| 863089|\n|2150495| 704198|\n|2402729|  19509|\n|1693173|  25699|\n| 756541|  68707|\n|2683189|1553647|\n|1126237| 128959|\n|2597998| 115729|\n|2042249| 772196|\n|2564105| 944193|\n|3079144| 438103|\n|1637980| 103885|\n|2108830| 213212|\n| 401313| 242653|\n| 686841| 627482|\n|  17845|  30042|\n|1079419|  65457|\n+-------+-------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "papers = spark.read.csv(\"wasb:///papers.tsv\",sep='\\t',schema = schema_papers)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "papers.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+--------------------+-----+-------+--------------------+\n| id|                name|venue|  pages|                 url|\n+---+--------------------+-----+-------+--------------------+\n|  0|Parallel Integer ...|    0|607-619|http://dx.doi.org...|\n|  1|Pattern Matching ...|    1|227-248|http://dx.doi.org...|\n|  2|NP-complete Probl...|    1|171-178|http://dx.doi.org...|\n|  3|On the Power of C...|    3|425-433|http://dx.doi.org...|\n|  4|Schnelle Multipli...|    4|395-398|http://dx.doi.org...|\n|  5|A characterizatio...|    5|  19-24|http://dx.doi.org...|\n|  6|The Derivation of...|    6|595-632|http://dx.doi.org...|\n|  7|Fifo Nets Without...|    7|  15-36|http://dx.doi.org...|\n|  8|On the Complement...|    8|297-305|http://dx.doi.org...|\n|  9|Equational weight...|    9|  29-52|http://dx.doi.org...|\n| 10|Merged processes:...|   10|307-330|http://dx.doi.org...|\n| 11|Verifying a simpl...|   11|199-228|http://dx.doi.org...|\n| 12|A Three-Stage Con...|    1|197-206|http://dx.doi.org...|\n| 13|The Expressive Po...|   13|447-452|http://dx.doi.org...|\n| 14|Calculi for Inter...|   14|707-737|http://dx.doi.org...|\n| 15|A Synthesis of Se...|   15|   1-30|http://dx.doi.org...|\n| 16|A Workload Model ...|   16|255-266|http://dx.doi.org...|\n| 17|Gray visiting Mot...|   17|793-811|http://link.sprin...|\n| 18|Trace- and failur...|   18|499-552|http://dx.doi.org...|\n| 19|Branching Process...|   19|277-298|http://dx.doi.org...|\n+---+--------------------+-----+-------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "venue = spark.read.csv('wasb:///venue.tsv',sep='\\t',schema = schema_venue)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "venue.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+--------------------+----+------+--------------+------+----+\n|     id|                name|year|school|        volume|number|type|\n+-------+--------------------+----+------+--------------+------+----+\n| 795226|                CoRR|2014|  null| abs/1409.0286|  null|   0|\n| 840379|Computational Bio...|2008|  null|            32|     1|   0|\n| 107610|Earth Science Inf...|2008|  null|             1|     1|   0|\n| 617896|              IJPEDS|2009|  null|            24|     4|   0|\n| 786188|                CoRR|2014|  null| abs/1409.0289|  null|   0|\n| 277767|EURASIP J. Wirele...|2012|  null|          2012|  null|   0|\n| 803082|                CoRR|2015|  null|abs/1506.07062|  null|   0|\n| 824640|                CoRR|2008|  null| abs/0812.0736|  null|   0|\n|1150895|Automatic Control...|2007|  null|            41|     3|   0|\n|1373655|J. of Management ...|2007|  null|            23|     4|   0|\n| 496575|Mathematical and ...|2009|  null|            49| 11-12|   0|\n| 170887|              IJTMCC|2014|  null|             2|     3|   0|\n| 642760|International Jou...|2015|  null|            29|     5|   0|\n| 810275|                CoRR|2014|  null| abs/1409.0280|  null|   0|\n| 828151|                CoRR|2014|  null| abs/1402.4678|  null|   0|\n| 851176|              IJHPCN|2015|  null|             8|     4|   0|\n|1392052|J. Electronic Ima...|2004|  null|            13|     2|   0|\n|  90908|Informatics in Ed...|2008|  null|             7|     2|   0|\n|1144593|SIAM J. Imaging S...|2008|  null|             1|     4|   0|\n| 809632|                CoRR|2011|  null| abs/1112.2000|  null|   0|\n+-------+--------------------+----+------+--------------+------+----+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "We created usable DataFrame so now, we can answer to the questions :", "cell_type": "markdown", "metadata": {}}, {"source": "### 1) Find the names of the top -k authors who have published the most in the DBLP dataset.", "cell_type": "markdown", "metadata": {}}, {"source": "For this task , k is an argument to your spark program.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "from pyspark.sql.functions import desc", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 15, "cell_type": "code", "source": "def top_k_authors(k):\n    \n    df = paperauths.join(authors,paperauths[\"authid\"] == authors[\"id\"]).groupBy(authors[\"id\"]).count()\n    \n    df2 = df.join(authors,\"id\").sort(desc(\"count\")).select(\"name\").show(k)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "top_k_authors(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------+\n|           name|\n+---------------+\n|H. Vincent Poor|\n|       Wei Wang|\n|      Yan Zhang|\n|        Wei Liu|\n|        Wen Gao|\n|   Philip S. Yu|\n|      Wei Zhang|\n|Thomas S. Huang|\n|      Yang Yang|\n|    Lajos Hanzo|\n+---------------+\nonly showing top 10 rows"}], "metadata": {"collapsed": false}}, {"source": "### 2) Find the set of authors who frequently write papers together.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "For this analytic task,you are expected to find the set of authors who have written at least X papers together , where X = 0.0001*total_num_papers.\n\nYour program will involve RDD/DataFrame operations as well as the FP growth package in MLlib for frequent pattern mining (unless you want to implement your own ).FP-  growth is an improvement of the Apriori algorithm ,which we learned in the lecture on \u201cScalable MachineLearning\u201d.Please output the frequent co-author lists.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "from pyspark.mllib.fpm import FPGrowth", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "from pyspark.sql import functions as F\n\ndf= paperauths.groupby(\"paperid\").agg(F.collect_set(\"authid\").alias(\"set_of_authors\")).sort(\"paperid\")\n\ndf.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+----------------+\n|paperid|  set_of_authors|\n+-------+----------------+\n|      0|             [0]|\n|      1|             [1]|\n|      2|          [2, 3]|\n|      3|             [4]|\n|      4|             [5]|\n|      5|             [6]|\n|      6|          [7, 8]|\n|      7|         [9, 10]|\n|      8|            [11]|\n|      9|    [12, 13, 14]|\n|     10|[15, 16, 17, 18]|\n|     11|            [19]|\n|     12|            [20]|\n|     13|        [21, 22]|\n|     14|            [23]|\n|     15|            [24]|\n|     16|    [27, 25, 26]|\n|     17|            [28]|\n|     18|    [30, 18, 29]|\n|     19|            [31]|\n+-------+----------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "For each paper, we find the id of the author(s) who written it.\n\nWe can notice that it is possible to have only one author who produced a paper. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "rdd = df.rdd", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": "rdd2 = rdd.map(lambda x : x[1])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "rdd2.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[[0], [1], [2, 3], [4], [5]]"}], "metadata": {"collapsed": false}}, {"source": "We created an Rdd in wich we have the sets of authors who worked on the same paper", "cell_type": "markdown", "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "model = FPGrowth.train(rdd2, minSupport=0.0001, numPartitions=10)\nresult = model.freqItemsets().filter(lambda x : len(x.items) > 1)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In our case, we want to find the set of authors who frequently write papers together. Thus, we have to take the set of authors with more than one author.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "result.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[FreqItemset(items=[125757, 11217], freq=425)]"}], "metadata": {"collapsed": false}}, {"source": "We just find one set of authors with more than one author which group authors who frequently write papers together.\n\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "rdd3 = result.map(lambda x : x.items)\nrdd3.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[[125757, 11217]]"}], "metadata": {"collapsed": false}}, {"source": "Now, we have to create a function in order to change the id lists of the authors into lists of author names.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "def convert_result(rdd):\n    rdd2 = rdd.collect()\n    l = []\n    i_1=0\n    for i in rdd2 :\n        l.append([])\n        for j in i:\n            name = authors.select(\"name\").where(authors[\"id\"] == j ).collect()\n            l[i_1].append(name[0][0])\n        i_1=i_1+1\n    return l", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 26, "cell_type": "code", "source": "convert_result(rdd3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[[u'Irith Pomeranz', u'Sudhakar M. Reddy']]"}], "metadata": {"collapsed": false}}, {"source": "Finally, we found that the authors **Irith Pomeranz** and **Sudhakar M.Reddy** frequently write papers together.", "cell_type": "markdown", "metadata": {}}, {"source": "### 3) Find the top-5 words whose frequency (in papers titles ) varies the most between year 2000 and 2015 for SIGMOD conferences. ", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "For each word ,illustrate the frequency per year.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "from pyspark.sql.functions import lower", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 28, "cell_type": "code", "source": "df = papers.join(venue,papers[\"venue\"] == venue[\"id\"]).where(venue[\"year\"] >= 2000).where( venue[\"year\"] <= 2015).where(venue[\"name\"].like(\"%SIGMOD Conference%\"))\ndf2 = df.withColumn(\"name_lower\", lower(papers[\"name\"])).select(\"name_lower\", venue[\"year\"])\ndf2.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+----+\n|          name_lower|year|\n+--------------------+----+\n|cougar: the netwo...|2002|\n|internet traffic ...|2000|\n|towards unified a...|2014|\n|datasift: a crowd...|2014|\n|pbir - perception...|2001|\n|databases on the ...|2007|\n|schema and ontolo...|2005|\n|provenance manage...|2006|\n|using spider: an ...|2006|\n|slq: a user-frien...|2014|\n|    lean middleware.|2005|\n|large graph proce...|2010|\n|nova: continuous ...|2011|\n|monitoring xml da...|2001|\n|schemascope: a sy...|2008|\n|sqlgraph: an effi...|2015|\n|the future of web...|2003|\n|from del.icio.us ...|2008|\n|reef: retainable ...|2015|\n|persistent data s...|2015|\n+--------------------+----+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 29, "cell_type": "code", "source": "from pyspark.sql.functions import split\n\ndf3 = df2.withColumn(\"name\", split(\"name_lower\", \"\\\\W+\")).select(\"name\",\"year\")\ndf3.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+----+\n|                name|year|\n+--------------------+----+\n|[cougar, the, net...|2002|\n|[internet, traffi...|2000|\n|[towards, unified...|2014|\n|[datasift, a, cro...|2014|\n|[pbir, perception...|2001|\n|[databases, on, t...|2007|\n|[schema, and, ont...|2005|\n|[provenance, mana...|2006|\n|[using, spider, a...|2006|\n|[slq, a, user, fr...|2014|\n|[lean, middleware, ]|2005|\n|[large, graph, pr...|2010|\n|[nova, continuous...|2011|\n|[monitoring, xml,...|2001|\n|[schemascope, a, ...|2008|\n|[sqlgraph, an, ef...|2015|\n|[the, future, of,...|2003|\n|[from, del, icio,...|2008|\n|[reef, retainable...|2015|\n|[persistent, data...|2015|\n+--------------------+----+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 30, "cell_type": "code", "source": "from pyspark.ml.feature import StopWordsRemover\n\nremover = StopWordsRemover(inputCol=\"name\", outputCol=\"name_filtered\")\ndf3_ = remover.transform(df3)\ndf3_.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+----+--------------------+\n|                name|year|       name_filtered|\n+--------------------+----+--------------------+\n|[cougar, the, net...|2002|[cougar, network,...|\n|[internet, traffi...|2000|[internet, traffi...|\n|[towards, unified...|2014|[towards, unified...|\n|[datasift, a, cro...|2014|[datasift, crowd,...|\n|[pbir, perception...|2001|[pbir, perception...|\n|[databases, on, t...|2007|  [databases, web, ]|\n|[schema, and, ont...|2005|[schema, ontology...|\n|[provenance, mana...|2006|[provenance, mana...|\n|[using, spider, a...|2006|[using, spider, e...|\n|[slq, a, user, fr...|2014|[slq, user, frien...|\n|[lean, middleware, ]|2005|[lean, middleware, ]|\n|[large, graph, pr...|2010|[large, graph, pr...|\n|[nova, continuous...|2011|[nova, continuous...|\n|[monitoring, xml,...|2001|[monitoring, xml,...|\n|[schemascope, a, ...|2008|[schemascope, sys...|\n|[sqlgraph, an, ef...|2015|[sqlgraph, effici...|\n|[the, future, of,...|2003|[future, web, ser...|\n|[from, del, icio,...|2008|[del, icio, us, x...|\n|[reef, retainable...|2015|[reef, retainable...|\n|[persistent, data...|2015|[persistent, data...|\n+--------------------+----+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "We use StopWordRemover to remove the worlds which are insignificant (for example : \"a\",\"for\"...) and which can skew our results", "cell_type": "markdown", "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": "df4 = df3_.withColumn('word', F.explode(F.col(\"name_filtered\"))).groupBy('word','year').count().sort('year', ascending=True)\ndf5 = df4.filter(df4[\"word\"] != \"\")\ndf5.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+----+-----+\n|       word|year|count|\n+-----------+----+-----+\n|approximate|2000|    2|\n|       auto|2000|    1|\n|        sql|2000|    2|\n|     design|2000|    1|\n|     spaces|2000|    1|\n| compressor|2000|    1|\n| attributes|2000|    1|\n|         rs|2000|    1|\n|       line|2000|    2|\n|incremental|2000|    1|\n|       main|2000|    1|\n|        aqr|2000|    1|\n| preserving|2000|    1|\n|  positions|2000|    1|\n| organizing|2000|    1|\n|       sets|2000|    2|\n|    rebuild|2000|    1|\n|  combining|2000|    1|\n|descriptors|2000|    1|\n|     memory|2000|    1|\n+-----------+----+-----+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "We can notice that when a word does not appear during a year, we do not have the value 0 assigned to this year.\n\nSo we need to solve this problem", "cell_type": "markdown", "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "rdd = df5.sort('year',ascending = True).rdd\n\nrdd.collect()[50]", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Row(word=u'memory', year=2000, count=1)"}], "metadata": {"collapsed": false}}, {"source": "Now, we will create a function which will return a list with each words and these correct frequencies (with 0 if it does not appear in a title during a year)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "def word_frequencies_by_year(rdd):\n    \n    rdd2 = rdd.collect()\n    \n    result = []\n    \n    did = []\n    \n    for i in range(len(rdd2)):\n        \n        if rdd2[i][0] not in did :\n        \n            result.append([ [rdd2[i][0]],[rdd2[i][2]] ]) \n            did.append(rdd2[i][0])\n            \n            if rdd2[i][1] > 2000 :\n                    \n                    x=2000\n                    \n                    while(x < rdd2[i][1]):\n                        \n                        result[-1][1].insert(0,0)\n                        x=x+1\n            \n            l_j = [i+1]\n            \n            for j in range(i,len(rdd2)-1):\n                \n                \n                if rdd2[i][0] == rdd2[j+1][0] and rdd2[i][1] < rdd2[j+1][1] :\n                    \n                    idx = l_j[-1]\n                    \n                    y = rdd2[j+1][1]\n                    \n                    while( y > rdd2[idx][1] + 1):\n                        \n                        result[-1][1].append(0) #we add 0 if the word does not appear a year\n                        \n                        y = y-1\n                        \n                    result[-1][1].append(rdd2[j+1][2])\n                    l_j.append(j+1)\n            \n            \n            while(len(result[-1][1]) < 16): #We add some 0 if the last year when the word occured was not 2015\n                \n                result[-1][1].append(0)\n                \n                \n                    \n    return result    ", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "result = word_frequencies_by_year(rdd)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 35, "cell_type": "code", "source": "result[87]", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[[u'functionality'], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]"}], "metadata": {"collapsed": false}}, {"source": "We added zero when a word does not occured in a paper title for the SIGMOD conferences during a year.\n\nFor each world, we have a list with its frequencies sorted by year between 2000 and 2015.\n\nNow, we can study the variance of each world.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": "import numpy as np", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 37, "cell_type": "code", "source": "rdd1 = spark.sparkContext.parallelize(result).map(lambda x : x).map(lambda x: (x[0][0], float( np.var(x[1])) ) )\nrdd2 = rdd1.sortBy(lambda x: x[1], ascending = False)\n\nrdd2.collect()[0]", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(u'data', 114.24609375)"}], "metadata": {"collapsed": false}}, {"execution_count": 38, "cell_type": "code", "source": "df = rdd2.toDF([\"word\",\"variance\"])\ndf.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------+------------+\n|  word|    variance|\n+------+------------+\n|  data|114.24609375|\n|   xml|       23.75|\n| query|   19.609375|\n|search| 16.33984375|\n|   big| 16.05859375|\n+------+------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"source": "Thus,**we found the the top-5 words whose frequency (in papers titles) varies the most between year 2000 and 2015 for SIGMOD conferences.**\n\nNow, we will plot the frenquencies of these words according to the year.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 39, "cell_type": "code", "source": "result2 = df.rdd.collect()\n\nwords = []\n\nfor i in range(5) :\n    \n    words.append(result2[i][0])\n    \nwords\n    \n\nrdd = spark.sparkContext.parallelize(result).map(lambda x : x).map(lambda x: (x[0][0], x[1] ) ).filter(lambda x : x[0] in words)\n\nresult3 = rdd.collect()\n\nresult3[0]", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(u'search', [1, 0, 4, 5, 2, 4, 10, 4, 8, 8, 11, 16, 7, 8, 10, 9])"}], "metadata": {"collapsed": false}}, {"source": "Now, we have in a rdd the words whose frequency varies the most between 2000 and 2015 for SIGMOD conferences and their frequencies.\n\nThus,we can plot the frequencies of each word according to the year.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 40, "cell_type": "code", "source": "\"\"\"\nimport matplotlib.pyplot as plt\n\nx = range(2000,2016)\n\ncolor= [\"b\",\"g\",\"r\",\"y\",\"k\"]\n\nfor i in range(len(result3)):\n    \n    plt.plot(x, result3[i][1] , color = color[i] , label = result3[i][0]) \n    \nplt.show()\n\n\"\"\"\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "'\\nimport matplotlib.pyplot as plt\\n\\nx = range(2000,2016)\\n\\ncolor= [\"b\",\"g\",\"r\",\"y\",\"k\"]\\n\\nfor i in range(len(result3)):\\n\\n    plt.plot(x, result3[i][1] , color = color[i] , label = result3[i][0]) \\n\\nplt.show()\\n\\n'"}], "metadata": {"collapsed": false}}, {"source": "**Comment :** I had some issues when I tried to plot a graph. Even the simplest graph can not be ploted using Microsoft Azure.\n\nI had this error : **This application failed to start because it could not find or load the Qt platform plugin \"xcb\"\nin \"\"**\n\nThus, please find in the above cell the piece of code (in comment) to plot the frequencies of the words whose frequency (in papers titles) varies the most between year 2000 and 2015 for SIGMOD conferences according to the year.", "cell_type": "markdown", "metadata": {}}, {"source": "### 4)Find the 20 clusters of topics from the titles of the papers published at SIGMOD conferences.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "**For each cluster,print the most representative words in the cluster.**", "cell_type": "markdown", "metadata": {}}, {"source": "For this question, the word **representative** can be interpreted in different ways.\n\nThus , we will print **the most representative title in each cluster (title which is the closest of the center of a cluster) in order to have the most representative words of the cluster.** \n\nThen, we will print **the most frequent words in each cluster**.", "cell_type": "markdown", "metadata": {}}, {"source": "### a) Most representative title in each cluster", "cell_type": "markdown", "metadata": {}}, {"execution_count": 41, "cell_type": "code", "source": "df = papers.join(venue,papers[\"venue\"] == venue[\"id\"]).where(venue[\"name\"].like(\"%SIGMOD Conference%\"))\ndf2 = df.withColumn(\"name_lower\", lower(papers[\"name\"])).select(\"name_lower\")\ndf2.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+\n|          name_lower|\n+--------------------+\n|cougar: the netwo...|\n|  magic is relevant.|\n|internet traffic ...|\n|nested historical...|\n|towards unified a...|\n|query optimizatio...|\n|datasift: a crowd...|\n|pbir - perception...|\n|the g+/graphlog v...|\n|on the power of a...|\n|databases on the ...|\n|schema and ontolo...|\n|provenance manage...|\n|using spider: an ...|\n|managing the data...|\n|information organ...|\n|record-boundary d...|\n|slq: a user-frien...|\n|an active mail sy...|\n|    lean middleware.|\n+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 42, "cell_type": "code", "source": "from pyspark.ml.feature import RegexTokenizer\n\nregex_tokenizer = RegexTokenizer(inputCol='name_lower', outputCol='name', pattern='\\\\W') \ndf3 = regex_tokenizer.transform(df2)\ndf3.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+\n|          name_lower|                name|\n+--------------------+--------------------+\n|cougar: the netwo...|[cougar, the, net...|\n|  magic is relevant.|[magic, is, relev...|\n|internet traffic ...|[internet, traffi...|\n|nested historical...|[nested, historic...|\n|towards unified a...|[towards, unified...|\n|query optimizatio...|[query, optimizat...|\n|datasift: a crowd...|[datasift, a, cro...|\n|pbir - perception...|[pbir, perception...|\n|the g+/graphlog v...|[the, g, graphlog...|\n|on the power of a...|[on, the, power, ...|\n|databases on the ...|[databases, on, t...|\n|schema and ontolo...|[schema, and, ont...|\n|provenance manage...|[provenance, mana...|\n|using spider: an ...|[using, spider, a...|\n|managing the data...|[managing, the, d...|\n|information organ...|[information, org...|\n|record-boundary d...|[record, boundary...|\n|slq: a user-frien...|[slq, a, user, fr...|\n|an active mail sy...|[an, active, mail...|\n|    lean middleware.|  [lean, middleware]|\n+--------------------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "In this task, we use the **RegexTokenizer** in order to split the text into words and **delete the empty charcters : \"\"**\n\nIn the previous question, we was able to delete these characters without RegexTokenizer because ,in our dataframe, we had single words but now, we have lists of words", "cell_type": "markdown", "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": "from pyspark.ml.feature import Word2Vec\nfrom pyspark.ml.clustering import KMeans", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 44, "cell_type": "code", "source": "remover = StopWordsRemover(inputCol=\"name\", outputCol=\"name_filtered\")\ndf4 = remover.transform(df3)\ndf4.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+--------------------+\n|          name_lower|                name|       name_filtered|\n+--------------------+--------------------+--------------------+\n|cougar: the netwo...|[cougar, the, net...|[cougar, network,...|\n|  magic is relevant.|[magic, is, relev...|   [magic, relevant]|\n|internet traffic ...|[internet, traffi...|[internet, traffi...|\n|nested historical...|[nested, historic...|[nested, historic...|\n|towards unified a...|[towards, unified...|[towards, unified...|\n|query optimizatio...|[query, optimizat...|[query, optimizat...|\n|datasift: a crowd...|[datasift, a, cro...|[datasift, crowd,...|\n|pbir - perception...|[pbir, perception...|[pbir, perception...|\n|the g+/graphlog v...|[the, g, graphlog...|[g, graphlog, vis...|\n|on the power of a...|[on, the, power, ...|[power, algebras,...|\n|databases on the ...|[databases, on, t...|    [databases, web]|\n|schema and ontolo...|[schema, and, ont...|[schema, ontology...|\n|provenance manage...|[provenance, mana...|[provenance, mana...|\n|using spider: an ...|[using, spider, a...|[using, spider, e...|\n|managing the data...|[managing, the, d...|[managing, data, ...|\n|information organ...|[information, org...|[information, org...|\n|record-boundary d...|[record, boundary...|[record, boundary...|\n|slq: a user-frien...|[slq, a, user, fr...|[slq, user, frien...|\n|an active mail sy...|[an, active, mail...|[active, mail, sy...|\n|    lean middleware.|  [lean, middleware]|  [lean, middleware]|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 45, "cell_type": "code", "source": "word2Vec = Word2Vec(minCount=0, inputCol=\"name_filtered\", outputCol=\"features\")\nmodel = word2Vec.fit(df4)\nresult = model.transform(df4)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 46, "cell_type": "code", "source": "kmeans = KMeans().setK(20).setSeed(20) #We take a precise value of setSeed() to have the same results when we re-run the kmeans.\nmodel = kmeans.fit(result)\n\npredictions = model.transform(result)\npredictions.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+--------------------+--------------------+----------+\n|          name_lower|                name|       name_filtered|            features|prediction|\n+--------------------+--------------------+--------------------+--------------------+----------+\n|cougar: the netwo...|[cougar, the, net...|[cougar, network,...|[0.01254908856935...|         8|\n|  magic is relevant.|[magic, is, relev...|   [magic, relevant]|[-8.1771460827440...|        11|\n|internet traffic ...|[internet, traffi...|[internet, traffi...|[-0.0061202097373...|         3|\n|nested historical...|[nested, historic...|[nested, historic...|[0.00902957407136...|         7|\n|towards unified a...|[towards, unified...|[towards, unified...|[0.02310320036485...|        15|\n|query optimizatio...|[query, optimizat...|[query, optimizat...|[0.01270825159735...|         6|\n|datasift: a crowd...|[datasift, a, cro...|[datasift, crowd,...|[0.00404311354504...|        14|\n|pbir - perception...|[pbir, perception...|[pbir, perception...|[0.00635837239678...|        14|\n|the g+/graphlog v...|[the, g, graphlog...|[g, graphlog, vis...|[0.00397301863413...|         0|\n|on the power of a...|[on, the, power, ...|[power, algebras,...|[0.00272238867667...|         3|\n|databases on the ...|[databases, on, t...|    [databases, web]|[0.02682498097419...|         5|\n|schema and ontolo...|[schema, and, ont...|[schema, ontology...|[0.00901391415391...|         7|\n|provenance manage...|[provenance, mana...|[provenance, mana...|[0.01296417141566...|         2|\n|using spider: an ...|[using, spider, a...|[using, spider, e...|[0.00239079416496...|        11|\n|managing the data...|[managing, the, d...|[managing, data, ...|[0.02455382576833...|         2|\n|information organ...|[information, org...|[information, org...|[0.00726414207019...|         0|\n|record-boundary d...|[record, boundary...|[record, boundary...|[0.00948462719097...|         7|\n|slq: a user-frien...|[slq, a, user, fr...|[slq, user, frien...|[0.01522616211635...|        19|\n|an active mail sy...|[an, active, mail...|[active, mail, sy...|[0.01440399225490...|         7|\n|    lean middleware.|  [lean, middleware]|  [lean, middleware]|[-0.0022621755924...|        11|\n+--------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "Now, we have to find the title which is the most representative in each cluster **or** to find the title (so the words) which is the closest of the centroid of each cluster.\n\nThus, we have to create a function in order to print the most representative title for each culster.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 47, "cell_type": "code", "source": "centers = model.clusterCenters()[0]\ncenters", "outputs": [{"output_type": "stream", "name": "stdout", "text": "array([ 0.00559503,  0.00728275,  0.00223948, -0.00771358, -0.00742338,\n        0.00056897,  0.01777088,  0.0033946 , -0.00608559,  0.01023632,\n        0.00917047,  0.00236614, -0.01549394, -0.00270655,  0.01490942,\n        0.00611227, -0.01141235, -0.01150733,  0.01718443, -0.0011045 ,\n        0.00068165,  0.01392751,  0.01173016, -0.01388611,  0.01228464,\n       -0.00286038,  0.0098411 , -0.00093999, -0.00180569, -0.00214292,\n        0.0021993 , -0.00194423,  0.00056261, -0.00099168,  0.01477347,\n       -0.00224042, -0.00240087,  0.00983683,  0.01526665, -0.003592  ,\n        0.00202762, -0.00255357, -0.01399132, -0.00144852,  0.00292356,\n        0.0168807 ,  0.01475725,  0.00030238,  0.00922787,  0.00362777,\n        0.01350636, -0.00084776,  0.00186446, -0.01513053,  0.0004463 ,\n       -0.0032675 ,  0.0043454 , -0.01254017,  0.00574641, -0.00443007,\n        0.0024009 ,  0.00466545,  0.00493961, -0.00580029,  0.01062113,\n       -0.00641125, -0.00842177, -0.0200881 ,  0.00580312,  0.00117386,\n        0.00623408,  0.00469291, -0.0152272 ,  0.00119851,  0.00587444,\n        0.00377294,  0.01491928,  0.01604268, -0.00747051, -0.01364483,\n        0.00148162, -0.0019038 ,  0.00606828,  0.00370108,  0.00128662,\n       -0.01257273, -0.00707583,  0.00144303, -0.00337368, -0.00472046,\n        0.0058425 ,  0.00311812,  0.00254877, -0.00184283,  0.0072152 ,\n        0.00470973, -0.00429877,  0.00836419,  0.01272431,  0.01089443])"}], "metadata": {"collapsed": false}}, {"source": "We will use the function **clusterCenters()** to have information on the center of a cluster.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 48, "cell_type": "code", "source": "import numpy as np\n\ndef get_closest(cluster,rdd,model):\n \n    rdd2= rdd.collect()\n    First = False\n     \n    for j in range(len(rdd2)) :\n            \n            if rdd2[j][2] == cluster :\n            \n                temp_dist = np.linalg.norm(rdd2[j][1] - model.clusterCenters()[cluster])\n            \n            \n                if First == False :\n                \n                    closest = temp_dist\n                    best = j\n                    First = True\n                \n            \n                if temp_dist < closest and First == True :\n                \n                    closest = temp_dist\n                    best = j\n    \n    return rdd2[best][0]", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 49, "cell_type": "code", "source": "rdd = predictions.select(\"name_filtered\",\"features\",\"prediction\").rdd\n\nget_closest(1,rdd,model)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[u'automated', u'partitioning', u'design', u'parallel', u'database', u'systems']"}], "metadata": {"collapsed": false}}, {"source": "We created a function which can print the selected words (we used StopWordsRemover) of the most representative title of a cluster.\n\nNow, we can use it on each cluster:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 50, "cell_type": "code", "source": "for i in range(20):\n\n    print \"The selected words of the most representative title of cluster\",i,\": \",get_closest(i,rdd,model),\"\\n\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The selected words of the most representative title of cluster 0 :  [u'left', u'bit', u'right', u'sparql', u'join', u'queries', u'optional', u'patterns', u'left', u'outer', u'joins'] \n\nThe selected words of the most representative title of cluster 1 :  [u'automated', u'partitioning', u'design', u'parallel', u'database', u'systems'] \n\nThe selected words of the most representative title of cluster 2 :  [u'graphical', u'xquery', u'aqualogic', u'data', u'services', u'platform'] \n\nThe selected words of the most representative title of cluster 3 :  [u'order', u'magnitude', u'advantage', u'tpc', u'c', u'though', u'massive', u'parallelism'] \n\nThe selected words of the most representative title of cluster 4 :  [u'generic', u'framework', u'monitoring', u'continuous', u'spatial', u'queries', u'moving', u'objects'] \n\nThe selected words of the most representative title of cluster 5 :  [u'databases', u'web'] \n\nThe selected words of the most representative title of cluster 6 :  [u'sensitivity', u'analysis', u'explanations', u'robust', u'query', u'evaluation', u'probabilistic', u'databases'] \n\nThe selected words of the most representative title of cluster 7 :  [u'aries', u'im', u'efficient', u'high', u'concurrency', u'index', u'management', u'method', u'using', u'write', u'ahead', u'logging'] \n\nThe selected words of the most representative title of cluster 8 :  [u'database', u'research', u'computer', u'games'] \n\nThe selected words of the most representative title of cluster 9 :  [u'kweelt', u'yet', u'another', u'framework', u'query', u'xml'] \n\nThe selected words of the most representative title of cluster 10 :  [u'exploiting', u'context', u'analysis', u'combining', u'multiple', u'entity', u'resolution', u'systems'] \n\nThe selected words of the most representative title of cluster 11 :  [u'tdp', u'optimal', u'latency', u'budget', u'allocation', u'strategy', u'crowdsourced', u'maximum', u'operations'] \n\nThe selected words of the most representative title of cluster 12 :  [u'automatic', u'discovery', u'language', u'models', u'text', u'databases'] \n\nThe selected words of the most representative title of cluster 13 :  [u'integration', u'heterogeneous', u'databases', u'without', u'common', u'domains', u'using', u'queries', u'based', u'textual', u'similarity'] \n\nThe selected words of the most representative title of cluster 14 :  [u'raft', u'work', u'speeding', u'mapreduce', u'applications', u'task', u'node', u'failures'] \n\nThe selected words of the most representative title of cluster 15 :  [u'dynamic', u'workload', u'driven', u'data', u'integration', u'tableau'] \n\nThe selected words of the most representative title of cluster 16 :  [u'balancing', u'push', u'pull', u'data', u'broadcast'] \n\nThe selected words of the most representative title of cluster 17 :  [u'dbsim', u'simulation', u'tool', u'predicting', u'database', u'performance'] \n\nThe selected words of the most representative title of cluster 18 :  [u'framework', u'enforcing', u'application', u'policies', u'database', u'systems'] \n\nThe selected words of the most representative title of cluster 19 :  [u'user', u'defined', u'aggregate', u'functions', u'bridging', u'theory', u'practice']"}], "metadata": {"collapsed": false}}, {"source": "Finally, we printed the most representative title of each cluster.\n\n**Warning** : It can take quite a bit of time.", "cell_type": "markdown", "metadata": {}}, {"source": "Now, we will print the five most frequent words in each cluster.", "cell_type": "markdown", "metadata": {}}, {"source": "### b) Most frequent words in each cluster", "cell_type": "markdown", "metadata": {}}, {"execution_count": 51, "cell_type": "code", "source": "for i in range(20):\n    \n    pred = predictions.filter(predictions[\"prediction\"] == i).select(\"name_filtered\")\n    rdd1 = pred.rdd.flatMap(lambda x: x).flatMap(lambda x : x).map(lambda x: (x,1)) #we access to each word and create a couple (word,1) to find the frequency \n    rdd2 = rdd1.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending = False)\n    \n    df = rdd2.toDF([\"words\",\"frequency\"])\n    print \"cluster\",i,\":\"\n    df.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "cluster 0 :\n+---------+---------+\n|    words|frequency|\n+---------+---------+\n|    query|       46|\n|efficient|       35|\n|   search|       27|\n|databases|       24|\n|    based|       19|\n+---------+---------+\nonly showing top 5 rows\n\ncluster 1 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|  database|       50|\n|   systems|       29|\n|  oriented|       15|\n|    object|       14|\n|management|       12|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 2 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|      data|      228|\n|  database|       33|\n|    system|       29|\n|management|       25|\n|processing|       17|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 3 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|     trees|        8|\n| prototype|        5|\n|         b|        5|\n|      case|        4|\n|algorithms|        4|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 4 :\n+-----------+---------+\n|      words|frequency|\n+-----------+---------+\n|    queries|       40|\n| processing|       23|\n|        web|       19|\n|distributed|       14|\n|  databases|       13|\n+-----------+---------+\nonly showing top 5 rows\n\ncluster 5 :\n+---------+---------+\n|    words|frequency|\n+---------+---------+\n|databases|       10|\n|     time|        8|\n|    query|        7|\n| oriented|        6|\n|   object|        6|\n+---------+---------+\nonly showing top 5 rows\n\ncluster 6 :\n+------------+---------+\n|       words|frequency|\n+------------+---------+\n|       query|       70|\n|   databases|       35|\n|optimization|       17|\n|  relational|       15|\n|      search|       15|\n+------------+---------+\nonly showing top 5 rows\n\ncluster 7 :\n+-----------+---------+\n|      words|frequency|\n+-----------+---------+\n|     system|       38|\n|      based|       19|\n| management|       17|\n|information|       13|\n|        web|       11|\n+-----------+---------+\nonly showing top 5 rows\n\ncluster 8 :\n+------------+---------+\n|       words|frequency|\n+------------+---------+\n|    database|      107|\n|      system|       24|\n|        data|       20|\n|      design|       11|\n|applications|       10|\n+------------+---------+\nonly showing top 5 rows\n\ncluster 9 :\n+---------+---------+\n|    words|frequency|\n+---------+---------+\n|    query|       61|\n|   search|       28|\n|efficient|       17|\n|databases|       13|\n|    based|       11|\n+---------+---------+\nonly showing top 5 rows\n\ncluster 10 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|processing|       21|\n|   systems|       20|\n|    system|       20|\n|       web|       18|\n|management|       17|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 11 :\n+---------+---------+\n|    words|frequency|\n+---------+---------+\n|    using|       14|\n|   system|       10|\n|    trees|       10|\n|      sql|        8|\n|efficient|        7|\n+---------+---------+\nonly showing top 5 rows\n\ncluster 12 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|   queries|       44|\n|processing|       26|\n|     large|       22|\n|      data|       21|\n|      time|       21|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 13 :\n+---------+---------+\n|    words|frequency|\n+---------+---------+\n|    query|       60|\n|databases|       55|\n|    based|       31|\n|   search|       29|\n|efficient|       24|\n+---------+---------+\nonly showing top 5 rows\n\ncluster 14 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|    system|       19|\n|     based|       15|\n| databases|       11|\n|       xml|       10|\n|algorithms|       10|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 15 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|      data|      148|\n|management|       22|\n|processing|       18|\n|      base|       15|\n|    system|       14|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 16 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|      data|      161|\n|management|       21|\n|    system|       19|\n|  database|       19|\n|       big|       12|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 17 :\n+----------+---------+\n|     words|frequency|\n+----------+---------+\n|  database|       83|\n|    system|       10|\n| deductive|        6|\n|    issues|        5|\n|management|        4|\n+----------+---------+\nonly showing top 5 rows\n\ncluster 18 :\n+--------+---------+\n|   words|frequency|\n+--------+---------+\n|database|       98|\n| systems|       29|\n|  system|       16|\n|  object|       15|\n|    data|       15|\n+--------+---------+\nonly showing top 5 rows\n\ncluster 19 :\n+-----------+---------+\n|      words|frequency|\n+-----------+---------+\n|    queries|       26|\n|  efficient|       20|\n|   networks|       20|\n|information|       20|\n|  databases|       18|\n+-----------+---------+\nonly showing top 5 rows"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "**Warning** : It can take quite a bit of time.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}